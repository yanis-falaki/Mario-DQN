{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gymnasium.wrappers import FrameStack, GrayScaleObservation, ResizeObservation, TransformObservation\n",
    "import retro\n",
    "import cv2\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import torch.nn as nn\n",
    "import torchvision.transforms as transforms\n",
    "from collections import deque\n",
    "import itertools\n",
    "import numpy as np\n",
    "import random\n",
    "\n",
    "# set up matplotlib\n",
    "is_ipython = 'inline' in matplotlib.get_backend()\n",
    "if is_ipython:\n",
    "    from IPython import display"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class QApproximator(nn.Module):\n",
    "    def __init__(self):\n",
    "        # Convolutional output dimensions formula (in each depth slice): W_new = (W-F + 2P)/S + 1 where W=input_shape, F=kernel_shape, P=padding_amount, S=stride_amount\n",
    "        super(QApproximator, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(in_channels=4, out_channels=16, kernel_size=8, stride=4) # Feature map size: 20\n",
    "        self.conv2 = nn.Conv2d(in_channels=16, out_channels=32, kernel_size=4, stride=2) # Feature map size: 9\n",
    "        self.fc1 = nn.Linear(in_features=32*9*9, out_features=256)\n",
    "        self.fc2 = nn.Linear(in_features=256, out_features=4)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.conv1(x))\n",
    "        x = F.relu(self.conv2(x))\n",
    "\n",
    "        x = x.view(-1, 32*9*9) # Flattening for fully connected layers\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "\n",
    "        return x\n",
    "    \n",
    "    def act(self, state, device):\n",
    "        state_tensor = torch.tensor(state, device=device, dtype=torch.float32)\n",
    "        q_values = self(state_tensor.unsqueeze(0))\n",
    "        max_q_index = torch.argmax(q_values, dim=1)[0]\n",
    "        action = max_q_index.detach().item()\n",
    "\n",
    "        return action\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper function to plot model performance during training\n",
    "def plot_durations(show_result=False, episode_durations=[]):\n",
    "    plt.figure(1)\n",
    "    durations_t = torch.tensor(episode_durations, dtype=torch.float)\n",
    "    if show_result:\n",
    "        plt.title('Result')\n",
    "    else:\n",
    "        plt.clf()\n",
    "        plt.title('Training...')\n",
    "    plt.xlabel('Episode')\n",
    "    plt.ylabel('Duration')\n",
    "    plt.plot(durations_t.numpy())\n",
    "    # Take 100 episode averages and plot them too\n",
    "    if len(durations_t) >= 100:\n",
    "        means = durations_t.unfold(0, 100, 1).mean(1).view(-1)\n",
    "        means = torch.cat((torch.zeros(99), means))\n",
    "        plt.plot(means.numpy())\n",
    "\n",
    "    plt.pause(0.001)  # pause a bit so that plots are updated\n",
    "    if is_ipython:\n",
    "        if not show_result:\n",
    "            display.display(plt.gcf())\n",
    "            display.clear_output(wait=True)\n",
    "        else:\n",
    "            display.display(plt.gcf())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "EPISODES = 100\n",
    "EPSILON_START = 1.0\n",
    "EPSILON_END = 0.02\n",
    "EPSILON_DECAY=15000\n",
    "BUFFER_SIZE = 50000\n",
    "BATCH_SIZE = 128\n",
    "GAMMA = 0.99\n",
    "LEARNING_RATE=5e-4\n",
    "TARGET_UPDATE_FREQ = 1000\n",
    "MIN_REPLAY_SIZE = 2000\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\") \n",
    "\n",
    "# Initialize experience replay buffer\n",
    "replay_buffer = deque(maxlen=BUFFER_SIZE)\n",
    "\n",
    "# Initialize Q and fixed target network\n",
    "QNet = QApproximator().to(device)\n",
    "TargNet = QApproximator().to(device).eval() # Set TargNet in eval mode as we never calculate gradients for it\n",
    "TargNet.load_state_dict(QNet.state_dict())\n",
    "\n",
    "optimizer = torch.optim.Adam(QNet.parameters(), lr=LEARNING_RATE)\n",
    "\n",
    "# Actions are: Run right, run right and jump, jump, run left\n",
    "actions = np.array([[0, 0, 0, 0, 0, 0, 0, 0, 1], [0, 0, 0, 0, 0, 0, 0, 1, 1], [0, 0, 0, 0, 0, 0, 0, 1, 0], [0, 0, 0, 0, 0, 0, 1, 0, 0]])\n",
    "\n",
    "episode_rewards = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Custom scenario.json which ends the episode when there are only 2 lives left rather than none\n",
    "env = retro.RetroEnv(game=\"SuperMarioBros-Nes\", scenario='./scenario.json', render_mode='human')\n",
    "\n",
    "# Modify observations to be preprocessed.\n",
    "env = ResizeObservation(env, (84, 84))\n",
    "env = GrayScaleObservation(env)\n",
    "env = TransformObservation(env, lambda obs: obs / 255.0)\n",
    "env = FrameStack(env, num_stack=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Storing the action and reward over 4 actions, as we're making a decision every 4 frames\n",
    "current_action = 0\n",
    "action_step = 0\n",
    "single_experience_buffer = [None, None, None, None, None]\n",
    "\n",
    "state, info = env.reset()\n",
    "for _ in range(MIN_REPLAY_SIZE):\n",
    "    if action_step % 4 != 0 and action_step % 4 != 3:\n",
    "        new_state, reward, terminated, truncated, info = env.step(actions[current_action])\n",
    "        action_step += 1\n",
    "        single_experience_buffer[2] += reward\n",
    "\n",
    "        if terminated or truncated:\n",
    "            single_experience_buffer[3] = terminated\n",
    "            single_experience_buffer[4] = new_state\n",
    "            replay_buffer.append(single_experience_buffer)\n",
    "            action_step = 0\n",
    "            env.reset()\n",
    "\n",
    "        continue\n",
    "\n",
    "    elif action_step % 4 == 3:\n",
    "        new_state, reward, terminated, truncated, info = env.step(actions[current_action])\n",
    "        single_experience_buffer[2] += reward\n",
    "        single_experience_buffer[3] = terminated\n",
    "        single_experience_buffer[4] = new_state\n",
    "        replay_buffer.append(single_experience_buffer)\n",
    "\n",
    "        action_step = 0\n",
    "\n",
    "        if terminated or truncated:\n",
    "            env.reset()\n",
    "            acton_step = 0\n",
    "        continue\n",
    "\n",
    "    current_action = random.choice([0, 1, 2, 3])\n",
    "    action_step += 1\n",
    "\n",
    "    new_state, reward, terminated, truncated, info = env.step(actions[current_action])\n",
    "    single_experience_buffer = [state, current_action, reward, None, None]\n",
    "\n",
    "    if truncated or terminated:\n",
    "        single_experience_buffer = [state, current_action, reward, terminated, new_state]\n",
    "        replay_buffer.append(single_experience_buffer)\n",
    "        action_step = 0\n",
    "\n",
    "        env.reset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def optimize_model():\n",
    "    experiences = random.sample(replay_buffer, BATCH_SIZE)\n",
    "\n",
    "    states = np.asarray([t[0] for t in experiences])\n",
    "    actions = np.asarray([t[1] for t in experiences])\n",
    "    rewards = np.asarray([t[2] for t in experiences])\n",
    "    terminated = np.asarray([t[3] for t in experiences])\n",
    "    new_states = np.asarray([t[4] for t in experiences])\n",
    "\n",
    "    states_t = torch.as_tensor(states, dtype=torch.float32, device=device)\n",
    "    actions_t = torch.as_tensor(actions, dtype=torch.int64, device=device).unsqueeze(-1)\n",
    "    rewards_t = torch.as_tensor(rewards, dtype=torch.float32, device=device).unsqueeze(-1)\n",
    "    terminated_t = torch.as_tensor(terminated, dtype=torch.float32, device=device).unsqueeze(-1)\n",
    "    new_states_t = torch.as_tensor(new_states, dtype=torch.float32, device=device)\n",
    "\n",
    "    # Compute Targets\n",
    "    target_q_values = TargNet(new_states_t)\n",
    "    max_target_q_values = target_q_values.max(dim=1, keepdim=True)[0]\n",
    "\n",
    "    targets = rewards_t + GAMMA * (1 - terminated_t) * max_target_q_values\n",
    "\n",
    "    q_values = QNet(states_t)\n",
    "    action_q_values = torch.gather(input=q_values, dim=1, index=actions_t)\n",
    "\n",
    "    loss = F.mse_loss(action_q_values, targets)\n",
    "\n",
    "    # Gradient descent\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[14], line 49\u001b[0m\n\u001b[1;32m     46\u001b[0m replay_buffer\u001b[38;5;241m.\u001b[39mappend(single_experience_buffer)\n\u001b[1;32m     48\u001b[0m QNet\u001b[38;5;241m.\u001b[39mtrain()\n\u001b[0;32m---> 49\u001b[0m \u001b[43moptimize_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     50\u001b[0m total_steps \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m     52\u001b[0m action_step \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n",
      "Cell \u001b[0;32mIn[13], line 4\u001b[0m, in \u001b[0;36moptimize_model\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21moptimize_model\u001b[39m():\n\u001b[1;32m      2\u001b[0m     experiences \u001b[38;5;241m=\u001b[39m random\u001b[38;5;241m.\u001b[39msample(replay_buffer, BATCH_SIZE)\n\u001b[0;32m----> 4\u001b[0m     states \u001b[38;5;241m=\u001b[39m \u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43masarray\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[43mt\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mexperiences\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      5\u001b[0m     actions \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39masarray([t[\u001b[38;5;241m1\u001b[39m] \u001b[38;5;28;01mfor\u001b[39;00m t \u001b[38;5;129;01min\u001b[39;00m experiences])\n\u001b[1;32m      6\u001b[0m     rewards \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39masarray([t[\u001b[38;5;241m2\u001b[39m] \u001b[38;5;28;01mfor\u001b[39;00m t \u001b[38;5;129;01min\u001b[39;00m experiences])\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 640x480 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "total_steps = 0\n",
    "\n",
    "# Storing the action and reward over 4 actions, as we're making a decision every 4 frames\n",
    "current_action = 0\n",
    "action_step = 0\n",
    "single_experience_buffer = [None, None, None, None, None]\n",
    "\n",
    "for episodes in range(EPISODES):\n",
    "    episode_reward = 0\n",
    "    state, info = env.reset()\n",
    "    for t in itertools.count():\n",
    "        # Update target network\n",
    "        if total_steps % TARGET_UPDATE_FREQ == 0:\n",
    "            TargNet.load_state_dict(QNet.state_dict())\n",
    "\n",
    "        # Handling the case where we don't make decisions\n",
    "        if action_step % 4 != 0 and action_step % 4 != 3:\n",
    "            new_state, reward, terminated, truncated, new_info = env.step(actions[current_action])\n",
    "            action_step += 1\n",
    "            single_experience_buffer[2] += reward\n",
    "\n",
    "            if terminated or truncated:\n",
    "                single_experience_buffer[3] = terminated\n",
    "                single_experience_buffer[4] = new_state\n",
    "                replay_buffer.append(single_experience_buffer)\n",
    "                action_step = 0\n",
    "\n",
    "                # Optimize model\n",
    "                QNet.train()\n",
    "                optimize_model()\n",
    "                total_steps += 1\n",
    "\n",
    "                episode_reward += single_experience_buffer[2]\n",
    "                episode_rewards.append(episode_reward)\n",
    "                plot_durations(show_result=False, episode_durations=episode_rewards)\n",
    "                break\n",
    "\n",
    "            state, info = new_state, new_info\n",
    "            continue\n",
    "\n",
    "        elif action_step % 4 == 3:\n",
    "            new_state, reward, terminated, truncated, new_info = env.step(actions[current_action])\n",
    "            single_experience_buffer[2] += reward\n",
    "            single_experience_buffer[3] = terminated\n",
    "            single_experience_buffer[4] = new_state\n",
    "            replay_buffer.append(single_experience_buffer)\n",
    "\n",
    "            QNet.train()\n",
    "            optimize_model()\n",
    "            total_steps += 1\n",
    "\n",
    "            action_step = 0\n",
    "\n",
    "            episode_reward += single_experience_buffer[2]\n",
    "            if terminated or truncated:\n",
    "                acton_step = 0\n",
    "                episode_rewards.append(episode_reward)\n",
    "                plot_durations(show_result=False, episode_durations=episode_rewards)\n",
    "                break\n",
    "\n",
    "            state, info = new_state, new_info\n",
    "            continue\n",
    "\n",
    "\n",
    "        # Select action\n",
    "        epsilon = np.interp(total_steps, [0, EPSILON_DECAY], [EPSILON_START, EPSILON_END])\n",
    "        rand_sample = random.random()\n",
    "        \n",
    "        if rand_sample <= epsilon:\n",
    "            action = random.choice([0, 1, 2, 3])\n",
    "        else:\n",
    "            QNet.eval()\n",
    "            current_action = QNet.act(state=state, device=device)\n",
    "\n",
    "        # Take action\n",
    "        new_state, reward, terminated, truncated, new_info = env.step(actions[current_action])\n",
    "        action_step += 1\n",
    "\n",
    "        single_experience_buffer = [state, action, reward, None, None]\n",
    "\n",
    "        # Append to experience buffer immediately if terminated or truncated\n",
    "        if terminated or truncated:\n",
    "            single_experience_buffer = [state, action, reward, terminated, new_state]\n",
    "            replay_buffer.append(single_experience_buffer)\n",
    "            action_step = 0\n",
    "\n",
    "            # Optimize model\n",
    "            QNet.train()\n",
    "            optimize_model()\n",
    "            total_steps += 1\n",
    "\n",
    "            episode_reward += single_experience_buffer[2]\n",
    "            episode_rewards.append(episode_reward)\n",
    "            plot_durations(show_result=False, episode_durations=episode_rewards)\n",
    "\n",
    "            break\n",
    "\n",
    "        state, info = new_state, new_info\n",
    "\n",
    "    if episodes % 5 == 0 and episodes != 0:\n",
    "        checkpoint = {\n",
    "            'model_state_dict': QNet.state_dict(),\n",
    "            'optimizer_state_dict': optimizer.state_dict(),\n",
    "            'total_steps': total_steps\n",
    "        }\n",
    "        torch.save(checkpoint, f'./model_checkpoint_episode_{episodes}.pt')\n",
    "\n",
    "\n",
    "torch.save(QNet.state_dict(), './mario-dqn-model.pt')\n",
    "torch.save(optimizer.state_dict(), './mario-dqn-optimizer.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env = retro.RetroEnv(game=\"SuperMarioBros-Nes\", scenario='./scenario.json', render_mode='human')\n",
    "\n",
    "# Modify observations to be preprocessed.\n",
    "env = ResizeObservation(env, (84, 84))\n",
    "env = GrayScaleObservation(env)\n",
    "env = TransformObservation(env, lambda obs: obs / 255.0)\n",
    "env = FrameStack(env, num_stack=4)\n",
    "\n",
    "loaded_model = QApproximator().to(device)\n",
    "checkpoint = torch.load('./model_checkpoint_episode_25.pt')\n",
    "loaded_model.load_state_dict(checkpoint[\"model_state_dict\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[17], line 8\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m(\u001b[38;5;28;01mTrue\u001b[39;00m):\n\u001b[1;32m      7\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m action_step \u001b[38;5;241m%\u001b[39m \u001b[38;5;241m4\u001b[39m \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m0\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m action_step \u001b[38;5;241m%\u001b[39m \u001b[38;5;241m4\u001b[39m \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m3\u001b[39m:\n\u001b[0;32m----> 8\u001b[0m         state, reward, terminated, truncated, info \u001b[38;5;241m=\u001b[39m \u001b[43menv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43mactions\u001b[49m\u001b[43m[\u001b[49m\u001b[43mcurrent_action\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      9\u001b[0m         action_step \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m     11\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m terminated \u001b[38;5;129;01mor\u001b[39;00m truncated:\n",
      "File \u001b[0;32m~/.pyenv/versions/3.9.8/envs/deep-learning-3.9.8/lib/python3.9/site-packages/gymnasium/wrappers/frame_stack.py:179\u001b[0m, in \u001b[0;36mFrameStack.step\u001b[0;34m(self, action)\u001b[0m\n\u001b[1;32m    170\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mstep\u001b[39m(\u001b[38;5;28mself\u001b[39m, action):\n\u001b[1;32m    171\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Steps through the environment, appending the observation to the frame buffer.\u001b[39;00m\n\u001b[1;32m    172\u001b[0m \n\u001b[1;32m    173\u001b[0m \u001b[38;5;124;03m    Args:\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    177\u001b[0m \u001b[38;5;124;03m        Stacked observations, reward, terminated, truncated, and information from the environment\u001b[39;00m\n\u001b[1;32m    178\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 179\u001b[0m     observation, reward, terminated, truncated, info \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43menv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43maction\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    180\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mframes\u001b[38;5;241m.\u001b[39mappend(observation)\n\u001b[1;32m    181\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mobservation(\u001b[38;5;28;01mNone\u001b[39;00m), reward, terminated, truncated, info\n",
      "File \u001b[0;32m~/.pyenv/versions/3.9.8/envs/deep-learning-3.9.8/lib/python3.9/site-packages/gymnasium/core.py:522\u001b[0m, in \u001b[0;36mObservationWrapper.step\u001b[0;34m(self, action)\u001b[0m\n\u001b[1;32m    518\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mstep\u001b[39m(\n\u001b[1;32m    519\u001b[0m     \u001b[38;5;28mself\u001b[39m, action: ActType\n\u001b[1;32m    520\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mtuple\u001b[39m[WrapperObsType, SupportsFloat, \u001b[38;5;28mbool\u001b[39m, \u001b[38;5;28mbool\u001b[39m, \u001b[38;5;28mdict\u001b[39m[\u001b[38;5;28mstr\u001b[39m, Any]]:\n\u001b[1;32m    521\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Modifies the :attr:`env` after calling :meth:`step` using :meth:`self.observation` on the returned observations.\"\"\"\u001b[39;00m\n\u001b[0;32m--> 522\u001b[0m     observation, reward, terminated, truncated, info \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43menv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43maction\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    523\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mobservation(observation), reward, terminated, truncated, info\n",
      "File \u001b[0;32m~/.pyenv/versions/3.9.8/envs/deep-learning-3.9.8/lib/python3.9/site-packages/gymnasium/core.py:522\u001b[0m, in \u001b[0;36mObservationWrapper.step\u001b[0;34m(self, action)\u001b[0m\n\u001b[1;32m    518\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mstep\u001b[39m(\n\u001b[1;32m    519\u001b[0m     \u001b[38;5;28mself\u001b[39m, action: ActType\n\u001b[1;32m    520\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mtuple\u001b[39m[WrapperObsType, SupportsFloat, \u001b[38;5;28mbool\u001b[39m, \u001b[38;5;28mbool\u001b[39m, \u001b[38;5;28mdict\u001b[39m[\u001b[38;5;28mstr\u001b[39m, Any]]:\n\u001b[1;32m    521\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Modifies the :attr:`env` after calling :meth:`step` using :meth:`self.observation` on the returned observations.\"\"\"\u001b[39;00m\n\u001b[0;32m--> 522\u001b[0m     observation, reward, terminated, truncated, info \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43menv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43maction\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    523\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mobservation(observation), reward, terminated, truncated, info\n",
      "File \u001b[0;32m~/.pyenv/versions/3.9.8/envs/deep-learning-3.9.8/lib/python3.9/site-packages/gymnasium/core.py:522\u001b[0m, in \u001b[0;36mObservationWrapper.step\u001b[0;34m(self, action)\u001b[0m\n\u001b[1;32m    518\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mstep\u001b[39m(\n\u001b[1;32m    519\u001b[0m     \u001b[38;5;28mself\u001b[39m, action: ActType\n\u001b[1;32m    520\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mtuple\u001b[39m[WrapperObsType, SupportsFloat, \u001b[38;5;28mbool\u001b[39m, \u001b[38;5;28mbool\u001b[39m, \u001b[38;5;28mdict\u001b[39m[\u001b[38;5;28mstr\u001b[39m, Any]]:\n\u001b[1;32m    521\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Modifies the :attr:`env` after calling :meth:`step` using :meth:`self.observation` on the returned observations.\"\"\"\u001b[39;00m\n\u001b[0;32m--> 522\u001b[0m     observation, reward, terminated, truncated, info \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43menv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43maction\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    523\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mobservation(observation), reward, terminated, truncated, info\n",
      "File \u001b[0;32m~/.pyenv/versions/3.9.8/envs/deep-learning-3.9.8/lib/python3.9/site-packages/retro/retro_env.py:212\u001b[0m, in \u001b[0;36mRetroEnv.step\u001b[0;34m(self, a)\u001b[0m\n\u001b[1;32m    209\u001b[0m rew, done, info \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcompute_step()\n\u001b[1;32m    211\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrender_mode \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhuman\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m--> 212\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrender\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    214\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m ob, rew, \u001b[38;5;28mbool\u001b[39m(done), \u001b[38;5;28;01mFalse\u001b[39;00m, \u001b[38;5;28mdict\u001b[39m(info)\n",
      "File \u001b[0;32m~/.pyenv/versions/3.9.8/envs/deep-learning-3.9.8/lib/python3.9/site-packages/retro/retro_env.py:254\u001b[0m, in \u001b[0;36mRetroEnv.render\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    251\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mretro\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mrendering\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m SimpleImageViewer\n\u001b[1;32m    253\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mviewer \u001b[38;5;241m=\u001b[39m SimpleImageViewer()\n\u001b[0;32m--> 254\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mviewer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mimshow\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimg\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    255\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mviewer\u001b[38;5;241m.\u001b[39misopen\n",
      "File \u001b[0;32m~/.pyenv/versions/3.9.8/envs/deep-learning-3.9.8/lib/python3.9/site-packages/retro/rendering.py:118\u001b[0m, in \u001b[0;36mSimpleImageViewer.imshow\u001b[0;34m(self, arr)\u001b[0m\n\u001b[1;32m    116\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mwindow\u001b[38;5;241m.\u001b[39mswitch_to()\n\u001b[1;32m    117\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mwindow\u001b[38;5;241m.\u001b[39mdispatch_events()\n\u001b[0;32m--> 118\u001b[0m \u001b[43mtexture\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mblit\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# draw\u001b[39;00m\n\u001b[1;32m    119\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mwindow\u001b[38;5;241m.\u001b[39mflip()\n",
      "File \u001b[0;32m~/.pyenv/versions/3.9.8/envs/deep-learning-3.9.8/lib/python3.9/site-packages/pyglet/image/__init__.py:1618\u001b[0m, in \u001b[0;36mTexture.blit\u001b[0;34m(self, x, y, z, width, height)\u001b[0m\n\u001b[1;32m   1616\u001b[0m glPushClientAttrib(GL_CLIENT_VERTEX_ARRAY_BIT)\n\u001b[1;32m   1617\u001b[0m glInterleavedArrays(GL_T4F_V4F, \u001b[38;5;241m0\u001b[39m, array)\n\u001b[0;32m-> 1618\u001b[0m \u001b[43mglDrawArrays\u001b[49m\u001b[43m(\u001b[49m\u001b[43mGL_QUADS\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m4\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1619\u001b[0m glPopClientAttrib()\n\u001b[1;32m   1620\u001b[0m glPopAttrib()\n",
      "File \u001b[0;32m~/.pyenv/versions/3.9.8/envs/deep-learning-3.9.8/lib/python3.9/site-packages/pyglet/gl/lib.py:87\u001b[0m, in \u001b[0;36merrcheck\u001b[0;34m(result, func, arguments)\u001b[0m\n\u001b[1;32m     83\u001b[0m \u001b[38;5;28;01mclass\u001b[39;00m \u001b[38;5;21;01mGLException\u001b[39;00m(\u001b[38;5;167;01mException\u001b[39;00m):\n\u001b[1;32m     84\u001b[0m     \u001b[38;5;28;01mpass\u001b[39;00m\n\u001b[0;32m---> 87\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21merrcheck\u001b[39m(result, func, arguments):\n\u001b[1;32m     88\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m _debug_gl_trace:\n\u001b[1;32m     89\u001b[0m         \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Storing the action and reward over 4 actions, as we're making a decision every 4 frames\n",
    "current_action = 0\n",
    "action_step = 0\n",
    "\n",
    "state, info = env.reset()\n",
    "while(True):\n",
    "    if action_step % 4 != 0 and action_step % 4 != 3:\n",
    "        state, reward, terminated, truncated, info = env.step(actions[current_action])\n",
    "        action_step += 1\n",
    "\n",
    "        if terminated or truncated:\n",
    "            action_step = 0\n",
    "            state, info = env.reset()\n",
    "\n",
    "        continue\n",
    "\n",
    "    elif action_step % 4 == 3:\n",
    "        state, reward, terminated, truncated, info = env.step(actions[current_action])\n",
    "        action_step = 0\n",
    "\n",
    "        if terminated or truncated:\n",
    "            state, info = env.reset()\n",
    "            acton_step = 0\n",
    "        continue\n",
    "\n",
    "    current_action = loaded_model.act(state, device=device)\n",
    "    action_step += 1\n",
    "\n",
    "    state, reward, terminated, truncated, info = env.step(actions[current_action])\n",
    "\n",
    "    if truncated or terminated:\n",
    "        action_step = 0\n",
    "        state, info = env.reset()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "deep-learning-3.9.8",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
